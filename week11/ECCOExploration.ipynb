{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eighteenth century word vectors\n",
    "\n",
    "To aid our discussion, here are some tools for analyzing Heuser's ECCO vectors.\n",
    "\n",
    "The source is the [eighteenth century collections online (ECCO)](https://www.gale.com/primary-sources/eighteenth-century-collections-online). You should be able to access specific words from this collection from a Cornell IP. There is also [ECCO TCP](https://quod.lib.umich.edu/e/ecco?key=author;page=browse;value=g), which is a smaller, more well-curated open-access segment. The vectors we have seem to have been on the larger, uncorrected OCR version: errors like the word *honefly* (what was this really?) do not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, sys, math\n",
    "from matplotlib import pyplot\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import display, clear_output, Markdown, Latex\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuser released the vectors he trained for his experiments. Here's the first 25,000 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "reverse_vocabulary = {}\n",
    "\n",
    "vector_filename = \"../data/ecco/ecco_vectors.vec\"\n",
    "\n",
    "with open(vector_filename) as infile:\n",
    "    \n",
    "    matrix_shape = [int(x) for x in infile.readline().split()]\n",
    "    \n",
    "    embeddings = numpy.zeros(matrix_shape)\n",
    "    \n",
    "    for line in infile:\n",
    "        fields = line.rstrip().split()\n",
    "        \n",
    "        word_id = len(vocabulary)\n",
    "        vocabulary.append(fields[0])\n",
    "        reverse_vocabulary[fields[0]] = word_id\n",
    "        \n",
    "        embeddings[word_id,:] = numpy.array([float(x) for x in fields[1:]])\n",
    "        \n",
    "normalizer = 1.0 / numpy.sqrt(numpy.sum(embeddings ** 2, axis=1))\n",
    "embeddings *= normalizer[:, numpy.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function returns the vector associated with a word. It finds the numeric ID for the string in the vocabulary array, and grabs the associated row from the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(word):\n",
    "    word_id = reverse_vocabulary[word]\n",
    "    return embeddings[word_id,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling this function returns a 300-dimensional vector. Here are the first 10 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector(\"king\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector(\"queen\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since anything we get from a call to `vector` is a `numpy` array, we can do mathematical operations on them, like subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = vector(\"king\") - vector(\"man\")\n",
    "diff[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest(v):\n",
    "    scores = embeddings.dot(v) / numpy.linalg.norm(v)\n",
    "    return sorted(zip(scores, vocabulary), reverse=True)\n",
    "\n",
    "def show_nearest(v, n=20):\n",
    "    markdown_table = \"|Cosine similarity | Word|\\n|---:|:---|\\n\"\n",
    "    sorted_words = nearest(v)\n",
    "    for score, word in sorted_words[:n]:\n",
    "        markdown_table += \"|{:.3f}|{}|\\n\".format(score, word)\n",
    "    markdown_table += \"| ... | ... | \"\n",
    "    for score, word in sorted_words[-n:]:\n",
    "        markdown_table += \"|{:.3f}|{}|\\n\".format(score, word)\n",
    "\n",
    "    display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the famous example of vector arithmetic providing analogies. What if you just do *king* + *woman*? What about just *king*? Or instead of *man* and *woman* use *he* and *she*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nearest(vector(\"king\") - vector(\"man\") + vector(\"woman\"), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a view at the vectors directly. This heatmap shows the first 100 dimensions for nine words: the first three are male royalty, the last three are female pronouns, and the middle three are female royalty. Can we spot dimensions (columns) that seem to code for royalness (or maybe personhood) and others that code for gender? (... maybe? it's not obvious to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_rows(word_list):\n",
    "    ids_array = numpy.array([reverse_vocabulary[w] for w in word_list])\n",
    "    return embeddings[ids_array,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"emperor\", \"prince\", \"queen\", \"empress\", \"princess\", \"she\", \"her\", \"hers\"]\n",
    "\n",
    "pyplot.figure(figsize=(14, 8))\n",
    "pyplot.xticks([])\n",
    "pyplot.yticks(range(len(words)), words)\n",
    "pyplot.imshow(get_word_rows(words))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an attempt to reproduce Heuser's plot showing simple and refined virtues/vices. The centering and distance calculation is my attempt to automate the \"put labels on words on the periphery\" aesthetic. Zoom in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embeddings.dot(vector(\"virtue\") - vector(\"vice\"))\n",
    "y = embeddings.dot(vector(\"simplicity\") - vector(\"refinement\"))\n",
    "\n",
    "x -= x.mean()\n",
    "y -= y.mean()\n",
    "x /= x.std()\n",
    "y /= y.std()\n",
    "\n",
    "pyplot.figure(figsize=(30,30))\n",
    "pyplot.scatter(x, y, alpha=0.3)\n",
    "for i in range(len(vocabulary)):\n",
    "    distance = 0.08 * (x[i]**2 + y[i]**2)\n",
    "    if numpy.random.random() < distance ** 3:\n",
    "        pyplot.text(x[i], y[i], vocabulary[i], alpha=0.8)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
