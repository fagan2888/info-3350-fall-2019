{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings\n",
    "\n",
    "In the previous section we discussed topic models, which find groups of words that tend to occur together. These clusters often correspond to recognizable themes, which is why we describe them as \"topics\". Topic models do not consider word order within documents.\n",
    "\n",
    "Word embeddings are a similar method for finding relationships between words. They represent each word as a vector in a space with usually 50-200 dimensions. Words $a$ and $b$ with vectors that are close are similar for one of these reasons:\n",
    "\n",
    "* Substitutability. If word $a$ appears in a sentence, you could replace it with word $b$ and not change the meaning that much.\n",
    "* Collocation. Word $a$ usually appears immediately before or after word $b$.\n",
    "\n",
    "Embedding algorithms pay attention to word order, but only within a small sliding window. We will discuss how this works more on Wednesday, for now we will just look at what we can get from embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, sys, math\n",
    "from matplotlib import pyplot\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import display, clear_output, Markdown, Latex\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compare word vectors from two collections:\n",
    "\n",
    "* English translations of Icelandic sagas\n",
    "* The same Old Bailey 1820s collection we looked at last week\n",
    "\n",
    "I preprocessed these in slightly different ways, see if you can tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "reverse_vocabulary = {}\n",
    "\n",
    "#vector_filename = \"../data/OldBailey/oldbailey.vec\"\n",
    "vector_filename = \"../data/Sagas/sagas_en.vec\"\n",
    "\n",
    "with open(vector_filename) as infile:\n",
    "    \n",
    "    matrix_shape = [int(x) for x in infile.readline().split()]\n",
    "    \n",
    "    embeddings = numpy.zeros(matrix_shape)\n",
    "    \n",
    "    for line in infile:\n",
    "        fields = line.rstrip().split()\n",
    "        \n",
    "        word_id = len(vocabulary)\n",
    "        vocabulary.append(fields[0])\n",
    "        reverse_vocabulary[fields[0]] = word_id\n",
    "        \n",
    "        embeddings[word_id,:] = numpy.array([float(x) for x in fields[1:]])\n",
    "\n",
    "normalizer = 1.0 / numpy.sqrt(numpy.sum(embeddings ** 2, axis=1))\n",
    "embeddings *= normalizer[:, numpy.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The vocabulary includes punctuation and \"end of sentence\" marker </s>\n",
    "\" \".join(vocabulary[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why linear algebra is great\n",
    "\n",
    "An embedding is really a matrix, with one row for each word. That means that anything we can do with matrices we can do with these embeddings.\n",
    "\n",
    "We'll start by looking for near neighbors by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest(query):\n",
    "    q_id = reverse_vocabulary[query]\n",
    "    scores = embeddings.dot(embeddings[q_id,:])\n",
    "    return sorted(zip(scores, vocabulary), reverse=True)\n",
    "\n",
    "def show_nearest(query, n=20):\n",
    "    markdown_table = \"|Cosine similarity | Word|\\n|---:|:---|\\n\"\n",
    "    sorted_words = nearest(query)\n",
    "    for score, word in sorted_words[:n]:\n",
    "        markdown_table += \"|{:.3f}|{}|\\n\".format(score, word)\n",
    "    display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nearest(\"horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll cluster words by their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 50\n",
    "clustering = KMeans(n_clusters=num_clusters).fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_vocab = numpy.array(vocabulary)\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_words = numpy_vocab[clustering.labels_ == cluster]\n",
    "    print(cluster, \" \".join(cluster_words[:12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use an SVD to visualize the dimensions of maximum variation, and look for semantic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, dimension_weights, Vt = numpy.linalg.svd(embeddings, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension is mostly just reporting word frequency. The remaining dimensions have weights that are pretty close, so we shouldn't expect the 2D plot to summarize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_weights[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = U[:,1]\n",
    "y = U[:,2]\n",
    "\n",
    "pyplot.figure(figsize=(30,30))\n",
    "pyplot.scatter(x, y, alpha=0.1)\n",
    "for i in range(0, 700):\n",
    "    pyplot.text(x[i], y[i], vocabulary[i])\n",
    "for i in range(701, 3000, 5):\n",
    "    pyplot.text(x[i], y[i], vocabulary[i])\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
