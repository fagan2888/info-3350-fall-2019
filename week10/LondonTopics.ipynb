{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on London court cases\n",
    "\n",
    "We're going to be using a topic model to explore transcripts from court cases in London from 1820-1830. A topic model is similar to a document clustering algorithm, but instead of grouping together documents we're going to group together word *tokens*. A document can thus \"belong\" to multiple topics, and a word can be present in multiple topics.\n",
    "\n",
    "This is a small portion of the Old Bailey corpus, originally from https://www.oldbaileyonline.org/, formatted and annotated by http://fedora.clarin-d.uni-saarland.de/oldbailey/.\n",
    "\n",
    "**This is real history. Some of the content of these cases might feel closer to your real life than other fictional works we've studied. I have made an effort to not include some of the more nightmarish cases, but be ready.**\n",
    "\n",
    "Some of you are having trouble installing or running the `cython` package. If so, find someone at your table who has got this working. Answer each question on your own, but work with your tablemate.\n",
    "\n",
    "I have made some changes to the code from Monday that should make it possible to run the model-training cell multiple times without \n",
    "\n",
    "* *Before* looking at documents, take a moment to think about your expectations. What crimes will people be charged with? What sort of evidence will be presented? What will punishments be? Write this **on paper** with your name and netid.\n",
    "\n",
    "Run the topic model. Sometimes we are interested in small words like \"the\" or \"and\". Here we're looking for more meaning-bearing words, so we're going to selectively remove small or overly frequent words from the collection. I have given you a minimal set of words to get you started.\n",
    "\n",
    "* Sometimes we are interested in small words like \"the\" or \"and\". Here we're looking for more meaning-bearing words, so we're going to selectively remove small or overly frequent words from the collection. Go to the file `data/OldBailey/stoplist.txt` and add words to the \"stoplist\" (words to ignore). Rerun your model. You may want to repeat this process several times, updating the stoplist based on your new results. Describe several cases of words where you were not sure whether to keep them or not. Why were you uncertain? How might your analysis change depending on whether you removed those words or not? Come to a consensus at your table about stoplist edits, so that your results will be comparable in the next sections.\n",
    "\n",
    "[Response here]\n",
    "\n",
    "* Compare your topics to others at your table. Find four topics that are similar across all models. Record at least  three variants for each similar topic. Use the `top_docs()` function to show the most prominent documents for these topics (save these executed cells). Describe the documents that have the largest proportion of this topic, and compare those documents to your table-mates. Are they similar or different?\n",
    "\n",
    "[Response here]\n",
    "\n",
    "* Read about Mr. Trust's mugging in `documents[780][\"original\"]`. I've given you a cell that shows the most represented topics for this document. Do these topics represent the content of the document? What do they include and what do they miss? How does these topics compare to your table-mates?\n",
    "\n",
    "[Response here]\n",
    "\n",
    "* Is this a useful way to look at a collection? What type of analysis does it support, and what would be difficult? What do you want to know about the Old Bailey corpus now, and what methods or tools would you use to find out more?\n",
    "\n",
    "[Response here]\n",
    "\n",
    "* Now that you have explored the documents through the topic model, what do you think of the original questions? What crimes were people  charged with? What sort of evidence was presented? What punishments were applied? (Write your answer **on paper** in class). \n",
    "\n",
    "[Response here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext cython\n",
    "\n",
    "import re, sys, random, math\n",
    "from collections import Counter\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import display, clear_output, Markdown, Latex\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "word_pattern = re.compile(\"\\w[\\w\\-\\']*\\w|\\w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/OldBailey\"\n",
    "\n",
    "num_topics = 30\n",
    "doc_smoothing = 0.5\n",
    "word_smoothing = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from cython.view cimport array as cvarray\n",
    "import numpy as np\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, long[:] doc_tokens, long[:] doc_topics, long[:] topic_changes, long[:] doc_topic_counts):\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.doc_topics = doc_topics\n",
    "        self.topic_changes = topic_changes\n",
    "        self.doc_topic_counts = doc_topic_counts\n",
    "\n",
    "cdef class TopicModel:\n",
    "    \n",
    "    cdef long[:] topic_totals\n",
    "    cdef long[:,:] word_topics\n",
    "    cdef int num_topics\n",
    "    cdef int vocab_size\n",
    "    \n",
    "    cdef double[:] topic_probs\n",
    "    cdef double[:] topic_normalizers\n",
    "    cdef float doc_smoothing\n",
    "    cdef float word_smoothing\n",
    "    cdef float smoothing_times_vocab_size\n",
    "    \n",
    "    documents = []\n",
    "    vocabulary = []\n",
    "    \n",
    "    def __init__(self, num_topics, vocabulary, doc_smoothing, word_smoothing):\n",
    "        self.num_topics = num_topics\n",
    "        self.vocabulary.extend(vocabulary)\n",
    "        self.vocab_size = len(vocabulary)\n",
    "        \n",
    "        self.doc_smoothing = doc_smoothing\n",
    "        self.word_smoothing = word_smoothing\n",
    "        self.smoothing_times_vocab_size = word_smoothing * self.vocab_size\n",
    "        \n",
    "        self.topic_totals = np.zeros(num_topics, dtype=int)\n",
    "        self.word_topics = np.zeros((self.vocab_size, num_topics), dtype=int)\n",
    "    \n",
    "    def clear_documents(self):\n",
    "        self.documents.clear()\n",
    "    \n",
    "    def add_document(self, doc):\n",
    "        cdef int word_id, topic\n",
    "        \n",
    "        self.documents.append(doc)\n",
    "        \n",
    "        for i in range(len(doc.doc_tokens)):\n",
    "            word_id = doc.doc_tokens[i]\n",
    "            topic = random.randrange(self.num_topics)\n",
    "            doc.doc_topics[i] = topic\n",
    "            \n",
    "            self.word_topics[word_id,topic] += 1\n",
    "            self.topic_totals[topic] += 1\n",
    "            doc.doc_topic_counts[topic] += 1\n",
    "            \n",
    "    def sample(self, iterations):\n",
    "        cdef int old_topic, new_topic, word_id, topic, i, doc_length\n",
    "        cdef double sampling_sum = 0\n",
    "        cdef double sample\n",
    "        cdef long[:] word_topic_counts\n",
    "        \n",
    "        cdef long[:] doc_tokens\n",
    "        cdef long[:] doc_topics\n",
    "        cdef long[:] doc_topic_counts\n",
    "        cdef long[:] topic_changes\n",
    "        \n",
    "        cdef double[:] uniform_variates\n",
    "        cdef double[:] topic_probs = np.zeros(self.num_topics, dtype=float)\n",
    "        cdef double[:] topic_normalizers = np.zeros(self.num_topics, dtype=float)\n",
    "        \n",
    "        for topic in range(self.num_topics):\n",
    "            topic_normalizers[topic] = 1.0 / (self.topic_totals[topic] + self.smoothing_times_vocab_size)\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            for document in self.documents:\n",
    "                doc_tokens = document.doc_tokens\n",
    "                doc_topics = document.doc_topics\n",
    "                doc_topic_counts = document.doc_topic_counts\n",
    "                topic_changes = document.topic_changes\n",
    "                \n",
    "                doc_length = len(document.doc_tokens)\n",
    "                uniform_variates = np.random.random_sample(doc_length)\n",
    "                \n",
    "                for i in range(doc_length):\n",
    "                    word_id = doc_tokens[i]\n",
    "                    old_topic = doc_topics[i]\n",
    "                    word_topic_counts = self.word_topics[word_id,:]\n",
    "        \n",
    "                    ## erase the effect of this token\n",
    "                    word_topic_counts[old_topic] -= 1\n",
    "                    self.topic_totals[old_topic] -= 1\n",
    "                    doc_topic_counts[old_topic] -= 1\n",
    "        \n",
    "                    topic_normalizers[old_topic] = 1.0 / (self.topic_totals[old_topic] + self.smoothing_times_vocab_size)\n",
    "        \n",
    "                    ###\n",
    "                    ### SAMPLING DISTRIBUTION\n",
    "                    ###\n",
    "        \n",
    "                    sampling_sum = 0.0\n",
    "                    for topic in range(self.num_topics):\n",
    "                        topic_probs[topic] = (doc_topic_counts[topic] + self.doc_smoothing) * (word_topic_counts[topic] + self.word_smoothing) * topic_normalizers[topic]\n",
    "                        sampling_sum += topic_probs[topic]\n",
    "\n",
    "                    sample = uniform_variates[i] * sampling_sum\n",
    "        \n",
    "                    new_topic = 0\n",
    "                    while sample > topic_probs[new_topic]:\n",
    "                        sample -= topic_probs[new_topic]\n",
    "                        new_topic += 1\n",
    "            \n",
    "                    ## add the effect of this token back in\n",
    "                    word_topic_counts[new_topic] += 1\n",
    "                    self.topic_totals[new_topic] += 1\n",
    "                    doc_topic_counts[new_topic] += 1\n",
    "                    topic_normalizers[new_topic] = 1.0 / (self.topic_totals[new_topic] + self.smoothing_times_vocab_size)\n",
    "\n",
    "                    doc_topics[i] = new_topic\n",
    "        \n",
    "                    if new_topic != old_topic:\n",
    "                        #pass\n",
    "                        topic_changes[i] += 1\n",
    "\n",
    "    def topic_words(self, int topic, n_words=12):\n",
    "        sorted_words = sorted(zip(self.word_topics[:,topic], self.vocabulary), reverse=True)\n",
    "        return \" \".join([w for x, w in sorted_words[:n_words]])\n",
    "\n",
    "    def print_all_topics(self):\n",
    "        for topic in range(self.num_topics):\n",
    "            print(topic, self.topic_words(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the stoplist file\n",
    "\n",
    "stoplist = set()\n",
    "with open(\"{}/stoplist.txt\".format(source_directory), encoding=\"utf-8\") as stop_reader:\n",
    "    for line in stop_reader:\n",
    "        line = line.rstrip()\n",
    "        stoplist.add(line)\n",
    "\n",
    "\n",
    "## Read the documents file\n",
    "        \n",
    "word_counts = Counter()\n",
    "documents = []\n",
    "\n",
    "for line in open(\"{}/documents.txt\".format(source_directory), encoding=\"utf-8\"):\n",
    "    #line = line.lower()\n",
    "    \n",
    "    tokens = word_pattern.findall(line)\n",
    "    \n",
    "    ## remove stopwords, short words, and upper-cased words\n",
    "    tokens = [w for w in tokens if not w in stoplist and len(w) >= 3 and not w[0].isupper()]\n",
    "    word_counts.update(tokens)\n",
    "    \n",
    "    doc_topic_counts = np.zeros(num_topics, dtype=int)\n",
    "    \n",
    "    documents.append({ \"original\": line, \"token_strings\": tokens, \"topic_counts\": doc_topic_counts })\n",
    "\n",
    "## Now that we're done reading from disk, we can count the total\n",
    "##  number of words.\n",
    "vocabulary = list(word_counts.keys())\n",
    "word_ids = { w: i for (i, w) in enumerate(vocabulary) }\n",
    "\n",
    "## With the vocabulary, go back and create arrays of numeric word IDs\n",
    "for document in documents:\n",
    "    tokens = document[\"token_strings\"]\n",
    "    doc_topic_counts = document[\"topic_counts\"]\n",
    "    \n",
    "    doc_tokens = np.ndarray(len(tokens), dtype=int)\n",
    "    doc_topics = np.ndarray(len(tokens), dtype=int)\n",
    "    topic_changes = np.zeros(len(tokens), dtype=int)\n",
    "    \n",
    "    for i, w in enumerate(tokens):\n",
    "        doc_tokens[i] = word_ids[w]\n",
    "        ## topics will be initialized by the model\n",
    "    \n",
    "    document[\"doc_tokens\"] = doc_tokens\n",
    "    document[\"doc_topics\"] = doc_topics\n",
    "    document[\"topic_changes\"] = topic_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell actually runs the model\n",
    "\n",
    "This may take some time to run. It will print \"Done!\" at the end.\n",
    "\n",
    "I've made some changes since Monday that will let you re-run this cell without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics, vocabulary, doc_smoothing, word_smoothing)\n",
    "\n",
    "## `documents` seems to be a class variable, not an object variable\n",
    "model.clear_documents()\n",
    "\n",
    "for document in documents:\n",
    "    document[\"topic_changes\"].fill(0)\n",
    "    document[\"topic_counts\"].fill(0)\n",
    "    c_doc = Document(document[\"doc_tokens\"], document[\"doc_topics\"], document[\"topic_changes\"], document[\"topic_counts\"])\n",
    "    model.add_document(c_doc)\n",
    "\n",
    "sampling_dist = np.zeros(num_topics, dtype=float)\n",
    "\n",
    "doc_topic_probs = np.zeros((len(model.documents), num_topics))\n",
    "word_topic_probs = np.zeros((len(vocabulary), num_topics))\n",
    "\n",
    "# Initial burn-in iterations\n",
    "for i in range(10): # using 500 iterations for faster stoplist curation\n",
    "    start = timer()\n",
    "    model.sample(50)\n",
    "    elapsed_time = timer() - start\n",
    "    \n",
    "    display(Markdown(\"### Iteration {}, {:.2f} seconds per iteration\".format((i+1) * 50, elapsed_time / 50)))\n",
    "    \n",
    "    table_markdown = \"### Iteration {}, {:.2f} seconds per iteration\\n\".format((i+1) * 50, elapsed_time / 50)\n",
    "    table_markdown += \"|Topic | Most likely words (descending)|\\n\"\n",
    "    table_markdown += \"|--|--|\\n\"\n",
    "    for topic in range(num_topics):\n",
    "        table_markdown += \"|{}|{}|\\n\".format(topic, model.topic_words(topic, 12))\n",
    "    \n",
    "    clear_output()\n",
    "    display(Markdown(table_markdown))\n",
    "        \n",
    "# Saved samples\n",
    "for i in range(5):\n",
    "    model.sample(10)\n",
    "    \n",
    "    for doc_id, doc in enumerate(model.documents):\n",
    "        for word_id, topic in zip(doc.doc_tokens, doc.doc_topics):\n",
    "            doc_topic_probs[doc_id,topic] += 1\n",
    "            word_topic_probs[word_id,topic] += 1\n",
    "\n",
    "            \n",
    "print(\"Done!\")\n",
    "            \n",
    "# Normalize\n",
    "doc_row_sums = doc_topic_probs.sum(axis=1)\n",
    "doc_topic_probs /= doc_row_sums[:,np.newaxis]\n",
    "\n",
    "word_col_sums = word_topic_probs.sum(axis=0)\n",
    "word_topic_probs /= word_col_sums[np.newaxis,:]\n",
    "\n",
    "topic_top_words = []\n",
    "for topic in range(num_topics):\n",
    "    sorted_words = sorted(zip(word_topic_probs[:,topic], vocabulary), reverse=True)\n",
    "    topic_top_words.append(\" \".join([w for x, w in sorted_words[:12]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing a document as a mixture of topics\n",
    "\n",
    "At the end of the training process I'm saving multiple samples and averaging over them to get better estimates of the probability of words in topics and the probability of topics in words.\n",
    "\n",
    "For convenience, I created an array `topic_top_words` that contains the top 12 most probable words in this averaged matrix.\n",
    "\n",
    "Let's look at one of the longer documents, describing an incident of violence and property crime. (Most proper names have been removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[780][\"original\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most probable topics in this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(doc_topic_probs[780,:], topic_top_words), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell shows the prevalence of each topic from the beginning of the work to the end.\n",
    "\n",
    "I'm including this for comparison to Pliny's encyclopedia. It looks quite different to me, and tells us something about the construction of the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(num_topics):\n",
    "    print(topic, model.topic_words(topic, n_words=6))\n",
    "    pyplot.plot(doc_topic_probs[:,topic])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell prints the documents with the largest proportion of a specified topic.\n",
    "\n",
    "Find a few topics that are of interest, and compare results around your table. Are topics that look similar from the perspective of their top words bringing together similar documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs(topic, n_docs=10):\n",
    "    for doc_id in np.argsort(-doc_topic_probs[:,topic])[:n_docs]:\n",
    "        print(\"{} {:.1f}% | {}\".format(doc_id, 100 * doc_topic_probs[doc_id,topic], documents[doc_id][\"original\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs(11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
