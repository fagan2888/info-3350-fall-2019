{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on Roman Science\n",
    "\n",
    "*Before you run anything, write down your answer:* What do you expect are the subjects of science during the Roman Empire?\n",
    "\n",
    "In this notebook we will be using a **topic model** to explore a Roman encyclopedia.\n",
    "\n",
    "There are three cells with questions towards the end of the notebook.\n",
    "\n",
    "Once you are done, write your answer again (what is Roman science), having looked at Pliny's encyclopedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext cython\n",
    "\n",
    "import re, sys, random, math\n",
    "from collections import Counter\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "word_pattern = re.compile(\"\\w[\\w\\-\\']*\\w|\\w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../data/PlinyTheElder\"\n",
    "\n",
    "num_topics = 30\n",
    "doc_smoothing = 0.5\n",
    "word_smoothing = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from cython.view cimport array as cvarray\n",
    "import numpy as np\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, long[:] doc_tokens, long[:] doc_topics, long[:] topic_changes, long[:] doc_topic_counts):\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.doc_topics = doc_topics\n",
    "        self.topic_changes = topic_changes\n",
    "        self.doc_topic_counts = doc_topic_counts\n",
    "\n",
    "cdef class TopicModel:\n",
    "    \n",
    "    cdef long[:] topic_totals\n",
    "    cdef long[:,:] word_topics\n",
    "    cdef int num_topics\n",
    "    cdef int vocab_size\n",
    "    \n",
    "    cdef double[:] topic_probs\n",
    "    cdef double[:] topic_normalizers\n",
    "    cdef float doc_smoothing\n",
    "    cdef float word_smoothing\n",
    "    cdef float smoothing_times_vocab_size\n",
    "    \n",
    "    documents = []\n",
    "    vocabulary = []\n",
    "    \n",
    "    def __init__(self, num_topics, vocabulary, doc_smoothing, word_smoothing):\n",
    "        self.num_topics = num_topics\n",
    "        self.vocabulary.extend(vocabulary)\n",
    "        self.vocab_size = len(vocabulary)\n",
    "        \n",
    "        self.doc_smoothing = doc_smoothing\n",
    "        self.word_smoothing = word_smoothing\n",
    "        self.smoothing_times_vocab_size = word_smoothing * self.vocab_size\n",
    "        \n",
    "        self.topic_totals = np.zeros(num_topics, dtype=int)\n",
    "        self.word_topics = np.zeros((self.vocab_size, num_topics), dtype=int)\n",
    "    \n",
    "    def add_document(self, doc):\n",
    "        cdef int word_id, topic\n",
    "        \n",
    "        self.documents.append(doc)\n",
    "        \n",
    "        for i in range(len(doc.doc_tokens)):\n",
    "            word_id = doc.doc_tokens[i]\n",
    "            topic = doc.doc_topics[i]\n",
    "            \n",
    "            self.word_topics[word_id,topic] += 1\n",
    "            self.topic_totals[topic] += 1\n",
    "            doc.doc_topic_counts[topic] += 1\n",
    "            \n",
    "    def sample(self, iterations):\n",
    "        cdef int old_topic, new_topic, word_id, topic, i, doc_length\n",
    "        cdef double sampling_sum = 0\n",
    "        cdef double sample\n",
    "        cdef long[:] word_topic_counts\n",
    "        \n",
    "        cdef long[:] doc_tokens\n",
    "        cdef long[:] doc_topics\n",
    "        cdef long[:] doc_topic_counts\n",
    "        cdef long[:] topic_changes\n",
    "        \n",
    "        cdef double[:] uniform_variates\n",
    "        cdef double[:] topic_probs = np.zeros(self.num_topics, dtype=float)\n",
    "        cdef double[:] topic_normalizers = np.zeros(self.num_topics, dtype=float)\n",
    "        \n",
    "        for topic in range(self.num_topics):\n",
    "            topic_normalizers[topic] = 1.0 / (self.topic_totals[topic] + self.smoothing_times_vocab_size)\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            for document in self.documents:\n",
    "                doc_tokens = document.doc_tokens\n",
    "                doc_topics = document.doc_topics\n",
    "                doc_topic_counts = document.doc_topic_counts\n",
    "                topic_changes = document.topic_changes\n",
    "                \n",
    "                doc_length = len(document.doc_tokens)\n",
    "                uniform_variates = np.random.random_sample(doc_length)\n",
    "                \n",
    "                for i in range(doc_length):\n",
    "                    word_id = doc_tokens[i]\n",
    "                    old_topic = doc_topics[i]\n",
    "                    word_topic_counts = self.word_topics[word_id,:]\n",
    "        \n",
    "                    ## erase the effect of this token\n",
    "                    word_topic_counts[old_topic] -= 1\n",
    "                    self.topic_totals[old_topic] -= 1\n",
    "                    doc_topic_counts[old_topic] -= 1\n",
    "        \n",
    "                    topic_normalizers[old_topic] = 1.0 / (self.topic_totals[old_topic] + self.smoothing_times_vocab_size)\n",
    "        \n",
    "                    ###\n",
    "                    ### SAMPLING DISTRIBUTION\n",
    "                    ###\n",
    "        \n",
    "                    sampling_sum = 0.0\n",
    "                    for topic in range(self.num_topics):\n",
    "                        topic_probs[topic] = (doc_topic_counts[topic] + self.doc_smoothing) * (word_topic_counts[topic] + self.word_smoothing) * topic_normalizers[topic]\n",
    "                        sampling_sum += topic_probs[topic]\n",
    "\n",
    "                    sample = uniform_variates[i] * sampling_sum\n",
    "        \n",
    "                    new_topic = 0\n",
    "                    while sample > topic_probs[new_topic]:\n",
    "                        sample -= topic_probs[new_topic]\n",
    "                        new_topic += 1\n",
    "            \n",
    "                    ## add the effect of this token back in\n",
    "                    word_topic_counts[new_topic] += 1\n",
    "                    self.topic_totals[new_topic] += 1\n",
    "                    doc_topic_counts[new_topic] += 1\n",
    "                    topic_normalizers[new_topic] = 1.0 / (self.topic_totals[new_topic] + self.smoothing_times_vocab_size)\n",
    "\n",
    "                    doc_topics[i] = new_topic\n",
    "        \n",
    "                    if new_topic != old_topic:\n",
    "                        #pass\n",
    "                        topic_changes[i] += 1\n",
    "\n",
    "    def topic_words(self, int topic, n_words=12):\n",
    "        sorted_words = sorted(zip(self.word_topics[:,topic], self.vocabulary), reverse=True)\n",
    "        return \" \".join([w for x, w in sorted_words[:n_words]])\n",
    "\n",
    "    def print_all_topics(self):\n",
    "        for topic in range(self.num_topics):\n",
    "            print(topic, self.topic_words(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the stoplist file\n",
    "\n",
    "stoplist = set()\n",
    "with open(\"{}/stoplist.txt\".format(source_directory), encoding=\"utf-8\") as stop_reader:\n",
    "    for line in stop_reader:\n",
    "        line = line.rstrip()\n",
    "        stoplist.add(line)\n",
    "\n",
    "\n",
    "## Read the documents file\n",
    "        \n",
    "word_counts = Counter()\n",
    "documents = []\n",
    "\n",
    "for line in open(\"{}/documents.txt\".format(source_directory), encoding=\"utf-8\"):\n",
    "    #line = line.lower()\n",
    "    \n",
    "    tokens = word_pattern.findall(line)\n",
    "    \n",
    "    ## remove stopwords, short words, and upper-cased words\n",
    "    tokens = [w for w in tokens if not w in stoplist and len(w) >= 3 and not w[0].isupper()]\n",
    "    word_counts.update(tokens)\n",
    "    \n",
    "    doc_topic_counts = np.zeros(num_topics, dtype=int)\n",
    "    \n",
    "    documents.append({ \"original\": line, \"token_strings\": tokens, \"topic_counts\": doc_topic_counts })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we're done reading from disk, we can count the total\n",
    "##  number of words.\n",
    "\n",
    "vocabulary = list(word_counts.keys())\n",
    "vocabulary_size = len(vocabulary)\n",
    "word_ids = { w: i for (i, w) in enumerate(vocabulary) }\n",
    "smoothing_times_vocab_size = word_smoothing * vocabulary_size\n",
    "\n",
    "#word_topics = np.zeros((len(vocabulary), num_topics), dtype=int)\n",
    "#topic_totals = np.zeros(num_topics, dtype=int)\n",
    "\n",
    "for document in documents:\n",
    "    tokens = document[\"token_strings\"]\n",
    "    doc_topic_counts = document[\"topic_counts\"]\n",
    "    \n",
    "    doc_tokens = np.ndarray(len(tokens), dtype=int)\n",
    "    doc_topics = np.ndarray(len(tokens), dtype=int)\n",
    "    topic_changes = np.zeros(len(tokens), dtype=int)\n",
    "    \n",
    "    for i, w in enumerate(tokens):\n",
    "        word_id = word_ids[w]\n",
    "        topic = random.randrange(num_topics)\n",
    "        \n",
    "        doc_tokens[i] = word_id\n",
    "        doc_topics[i] = topic\n",
    "        \n",
    "        ## Update counts: \n",
    "        #word_topics[word_id][topic] += 1\n",
    "        #topic_totals[topic] += 1\n",
    "        #doc_topic_counts[topic] += 1\n",
    "    \n",
    "    document[\"doc_tokens\"] = doc_tokens\n",
    "    document[\"doc_topics\"] = doc_topics\n",
    "    document[\"topic_changes\"] = topic_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell actually runs the model\n",
    "\n",
    "This may take some time to run. It will print \"Done!\" at the end.\n",
    "\n",
    "* Do the topics you see make sense? Which ones are most meaningful, and which ones are least meaningful? Copy specific examples.\n",
    "\n",
    "* Are the topics consistent? Compare your results to others at your table. Write specific examples of similar and different (but recognizably related) versions of topics.\n",
    "\n",
    "[Answer below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics, vocabulary, doc_smoothing, word_smoothing)\n",
    "\n",
    "for document in documents:\n",
    "    c_doc = Document(document[\"doc_tokens\"], document[\"doc_topics\"], document[\"topic_changes\"], document[\"topic_counts\"])\n",
    "    model.add_document(c_doc)\n",
    "\n",
    "sampling_dist = np.zeros(num_topics, dtype=float)\n",
    "#topic_normalizers = np.zeros(num_topics, dtype=float)\n",
    "#for topic in range(num_topics):\n",
    "#    topic_normalizers[topic] = 1.0 / (topic_totals[topic] + smoothing_times_vocab_size)\n",
    "\n",
    "doc_topic_probs = np.zeros((len(model.documents), num_topics))\n",
    "word_topic_probs = np.zeros((len(vocabulary), num_topics))\n",
    "\n",
    "# Initial burn-in iterations\n",
    "for i in range(20):\n",
    "    start = timer()\n",
    "    model.sample(50)\n",
    "    elapsed_time = timer() - start\n",
    "    print(\"Iteration {}, {:.2f} seconds per iteration\".format(i * 50, elapsed_time / 50))\n",
    "    model.print_all_topics()\n",
    "    print()\n",
    "\n",
    "# Saved samples\n",
    "for i in range(5):\n",
    "    model.sample(10)\n",
    "    \n",
    "    for doc_id, doc in enumerate(model.documents):\n",
    "        for word_id, topic in zip(doc.doc_tokens, doc.doc_topics):\n",
    "            doc_topic_probs[doc_id,topic] += 1\n",
    "            word_topic_probs[word_id,topic] += 1\n",
    "\n",
    "            \n",
    "print(\"Done!\")\n",
    "            \n",
    "# Normalize\n",
    "doc_row_sums = doc_topic_probs.sum(axis=1)\n",
    "doc_topic_probs /= doc_row_sums[:,np.newaxis]\n",
    "\n",
    "word_col_sums = word_topic_probs.sum(axis=0)\n",
    "word_topic_probs /= word_col_sums[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell shows the prevalence of each topic from the beginning of the work to the end.\n",
    "\n",
    "* Which topics are most concentrated? Which topics are most dispersed through the work? How does this concentration/dispersion relate to their interpretability?\n",
    "\n",
    "* What can you infer about the structure of Pliny's encyclopedia?\n",
    "\n",
    "[Answer below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(num_topics):\n",
    "    print(topic, model.topic_words(topic, n_words=6))\n",
    "    pyplot.plot(doc_topic_probs[:,topic])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell prints the documents with the largest proportion of a specified topic.\n",
    "\n",
    "* Does looking at example documents help explain what the model is finding? Provide positive and negative examples if possible. Add cells to look at at least five topics.\n",
    "\n",
    "[Answer below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs(topic, n_docs=10):\n",
    "    for doc_id in np.argsort(-doc_topic_probs[:,topic])[:n_docs]:\n",
    "        print(doc_id, doc_topic_probs[doc_id,topic], documents[doc_id][\"original\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
